{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab2e41f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Sample                                               Text  Label\n",
      "1     10087  truth texas   republican president donald trum...      1\n",
      "2     10113  putin stressing tf hearing spilled bunch paint...      1\n",
      "3     10203  ip interference ukraine lead consequences ve s...      0\n",
      "4     10256    carolina forward thread forwardcarolina sena...      0\n",
      "5     10270             s fuck putin means everybody wants sex      1\n",
      "...     ...                                                ...    ...\n",
      "4039  99632  protestors world rally support ukraine storyfulap      0\n",
      "4040  99658  rt live q   mar     home   russia   fsu wwiii ...      0\n",
      "4041  99773  ragingeve costa mesa couple barely escape ukra...      0\n",
      "4042  99882  sign contribute guardian   years news opinion ...      0\n",
      "4043  99927  rob lee ralee colonel alexey gorobets commande...      0\n",
      "\n",
      "[4043 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hate_stA_train = pd.read_csv(\".\\OnlyText\\stA\\hate_stA_train.csv\")\n",
    "nohate_stA_train = pd.read_csv(\".\\OnlyText\\stA\\put.csv\")\n",
    "stA_eval = pd.read_csv(\".\\OnlyText\\stA\\stA_eval.csv\")\n",
    "stA_test = pd.read_csv(\".\\OnlyText\\stA\\stA_test.csv\")\n",
    "eval_subtaskA = pd.read_csv(\".\\OnlyText\\stA\\eval_subtaskA.csv\")\n",
    "\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "'''\n",
    "def preprocess_synopsis(text):\n",
    "    text.replace(\"\\n\",\" \")\n",
    "    text.replace(\"\\r\",\"\")\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower()).replace('\\n', '').replace('\\r', '')\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_stop]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "'''\n",
    "\n",
    "\n",
    "problematic_indices_eval = [\n",
    "    39391,\n",
    "48843,\n",
    "94515,\n",
    "81181,\n",
    "69034,\n",
    "47368,\n",
    "29455,\n",
    "56174,\n",
    "57069,\n",
    "74368,\n",
    "67470,\n",
    "73193,\n",
    "50784,\n",
    "66268,\n",
    "84383,\n",
    "98442,\n",
    "99385\n",
    "]\n",
    "\n",
    "def labeler(sample_id, df):\n",
    "    for column, row in df.iterrows():\n",
    "        if str(row['index']) == str(sample_id):\n",
    "            a = int(row['Label'])\n",
    "            if a == 1:\n",
    "                return 1 #'hate speech'\n",
    "            else:\n",
    "                return 0 #'no hate speech'\n",
    "    print(\"Function failed\")\n",
    "\n",
    "training_data = pd.DataFrame(columns=['Sample', 'Text', 'Label'])\n",
    "hate = 1# 'hate speech' #1\n",
    "no_hate = 0#'no hate speech' #0\n",
    "count = 1\n",
    "for column, row in stA_eval.iterrows():\n",
    "    sample_id = row['filename'].replace(\"/content/drive/MyDrive/CASE2023_Task4/CASE2023_TASK4_EvalData/subtaskA/\",\"\").replace(\".jpg\",\"\")\n",
    "    sample_id_new = sample_id\n",
    "    if int(sample_id) in problematic_indices_eval:\n",
    "        sample_id_new = sample_id+\"(1)\"\n",
    "    sample_text = row['text']\n",
    "    sample_text = preprocess_synopsis(sample_text)\n",
    "    training_data.loc[count] = [sample_id_new, sample_text, labeler(sample_id, eval_subtaskA)]\n",
    "    count+=1\n",
    "for column, row in hate_stA_train.iterrows():\n",
    "    sample_id = row['filename'].replace(\"/content/drive/MyDrive/CASE2023_Task4/CASE2023_TASK4_TrainData/subTaskA/Hate Speech/\",\"\").replace(\".jpg\",\"\")\n",
    "    sample_text = row['text']\n",
    "    sample_text = preprocess_synopsis(sample_text)\n",
    "    training_data.loc[count] = [sample_id, sample_text, hate]\n",
    "    count+=1\n",
    "for column, row in nohate_stA_train.iterrows():\n",
    "    sample_id = row['filename'].replace(\"/content/drive/MyDrive/CASE2023_Task4/CASE2023_TASK4_TrainData/subTaskA/No Hate Speech/\",\"\").replace(\".jpg\",\"\")\n",
    "    sample_text = row['text']\n",
    "    sample_text = preprocess_synopsis(sample_text)\n",
    "    training_data.loc[count] = [sample_id, sample_text, no_hate]\n",
    "    count+=1\n",
    "\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60f1aa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sample                                               Text\n",
      "1    10113    When you keep all the good rations for yourself\n",
      "2    10165  Media 9:28 1 Tucker Carlson, downplaying Russi...\n",
      "3    10287  Andreeva Bay nuclear waste storage Bolshaya Lo...\n",
      "4    10443  So many electricians in the Ukraine but no ele...\n",
      "5    10532  Daily Mail MORE STORIES Q Russia has fired 'ab...\n",
      "..     ...                                                ...\n",
      "439  99353   UKRAINIAN RESISTANCE TO RUSSIA imgflip.com A30 $\n",
      "440  99743  Patricia Arquette @PattyArquette Well for Lord...\n",
      "441  99744  4:19 1 Amazon's response to the situation in U...\n",
      "442  99764  Fox News just asked Trump what he'd do differe...\n",
      "443  99828  ALL I SAID WAS... NO CHANCE OF WAR IN UKRAINE ...\n",
      "\n",
      "[443 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "for column, row in training_data.iterrows():\n",
    "    if row['Label'] == 0:\n",
    "        print(row['Text'])\n",
    "        break\n",
    "\n",
    "'Ip Interference in Ukraine move would lead to consequences you've never seen: Putin As he announced a military operation in Ukraine, Russian President Vladimir Putin warned other countries that any attempt to interfere with the Russian action would lead to \"consequences you have never seen\". He said the attack was needed to protect civilians in eastern Ukraine, a claim that the US had predicted he would falsely make to justify an invasion. short by Ankush Verma / 24 Feb, 2022 GET IT ON Google Play Download on the App Store inshorts'\n",
    "'''\n",
    "\n",
    "test_datas = pd.DataFrame(columns=['Sample', 'Text'])\n",
    "count = 1\n",
    "for column, row in stA_test.iterrows():\n",
    "    sample_id = row['filename'].replace(\"/content/drive/MyDrive/CASE2023_Task4/CASE2023_TASK4_TestData/subtaskA/\",\"\").replace(\".jpg\",\"\")\n",
    "    sample_text = row['text']\n",
    "    test_datas.loc[count] = [sample_id, sample_text]\n",
    "    count+=1\n",
    "    \n",
    "print(test_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0365fa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CHQ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\CHQ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5680123288643184, Test Accuracy = 0.7812113720642769, F1 Score = 0.7844092570036539\n",
      "Epoch 2: Train Loss = 0.40881286293561825, Test Accuracy = 0.7725587144622992, F1 Score = 0.7745098039215685\n",
      "Epoch 3: Train Loss = 0.26170835863303377, Test Accuracy = 0.8034610630407911, F1 Score = 0.8199320498301246\n",
      "Epoch 4: Train Loss = 0.1940963120667695, Test Accuracy = 0.8220024721878862, F1 Score = 0.8444924406047516\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "\n",
    "# Step 1: Prepare the environment\n",
    "df = training_data\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Step 2: Tokenize the text\n",
    "def tokenize_text(text):\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=256,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoded['input_ids'], encoded['attention_mask']\n",
    "\n",
    "train_tokens = train_df['Text'].apply(tokenize_text)\n",
    "test_tokens = test_df['Text'].apply(tokenize_text)\n",
    "\n",
    "# Step 3: Prepare the data for BERT\n",
    "train_inputs = torch.cat([token[0] for token in train_tokens])\n",
    "train_masks = torch.cat([token[1] for token in train_tokens])\n",
    "train_labels = torch.tensor(train_df['Label'].values)\n",
    "\n",
    "test_inputs = torch.cat([token[0] for token in test_tokens])\n",
    "test_masks = torch.cat([token[1] for token in test_tokens])\n",
    "test_labels = torch.tensor(test_df['Label'].values)\n",
    "\n",
    "# Step 4: Fine-tune BERT for classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[0]\n",
    "        attention_mask = batch[1]\n",
    "        labels = batch[2]\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[0]\n",
    "        attention_mask = batch[1]\n",
    "        labels = batch[2]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        true_labels.extend(labels.numpy())\n",
    "        predicted_labels.extend(predicted.numpy())\n",
    "\n",
    "    accuracy = total_correct / len(test_df)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}: Train Loss = {avg_train_loss}, Test Accuracy = {accuracy}, F1 Score = {f1}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f4fd9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text):\n",
    "    inputs = tokenize_text(text)\n",
    "    input_ids = inputs[0]\n",
    "    attention_mask = inputs[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs[0]\n",
    "\n",
    "    _, predicted_label = torch.max(logits, dim=1)\n",
    "    return predicted_label.item()\n",
    "\n",
    "predicciones_chidas = []\n",
    "for column, row in test_datas.iterrows():\n",
    "    sample = row['Sample']\n",
    "    texto = row['Text']\n",
    "    predicted_label = predict_text(texto)\n",
    "    linea = '{\"index\": '+ str(sample)+ ', \"prediction\": '+ str(predicted_label)+ '}'\n",
    "    predicciones_chidas.append(linea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "915665ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "jason = open('BERT_prepros_submission.json', 'w')\n",
    "for i in range(len(predicciones_chidas)-1):\n",
    "    jason.write(predicciones_chidas[i])\n",
    "    jason.write('\\n')\n",
    "jason.write(predicciones_chidas[-1])\n",
    "jason.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
