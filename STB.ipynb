{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab2e41f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Sample                                               Text  Label\n",
      "1     10757  breaking news american democrat blames putins ...      2\n",
      "2     10832  nt stand ukrainians brave resistance putinist ...      2\n",
      "3     11097  ship catastrophic accident sank response crews...      0\n",
      "4     11140  fox news channel tuckers thoughts russias acti...      0\n",
      "5     11967  nt know going happen pretty good idea putin go...      0\n",
      "...     ...                                                ...    ...\n",
      "2182  99524               live farming simulator   starecatcom      2\n",
      "2183  99587  loses small city ukraine k took early loses   ...      2\n",
      "2184  99679                 kyiv towing collect tank big small      2\n",
      "2185  99712  vere hems   valin saman san tit kyiv makes nig...      2\n",
      "2186  99727  donbass civilians constantly attacked ukraine ...      2\n",
      "\n",
      "[2186 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_synopsis(text):\n",
    "    text.replace(\"\\n\",\" \")\n",
    "    text.replace(\"\\r\",\"\")\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower()).replace('\\n', '').replace('\\r', '')\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_stop]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "community_stB_train = pd.read_csv(\".\\OnlyText\\stB\\community_stB_train.csv\")\n",
    "individual_stB_train = pd.read_csv(\".\\OnlyText\\stB\\individual_stB_train.csv\")\n",
    "org_stB_train = pd.read_csv(\".\\OnlyText\\stB\\org_stB_train.csv\")\n",
    "stB_eval = pd.read_csv(\".\\OnlyText\\stB\\stB_eval.csv\")\n",
    "stB_test = pd.read_csv(\".\\OnlyText\\stB\\stB_test.csv\")\n",
    "eval_subtaskB = pd.read_csv(\".\\OnlyText\\stB\\eval_subtaskB.csv\")\n",
    "\n",
    "def labeler(sample_id, df):\n",
    "    for column, row in df.iterrows():\n",
    "        if str(row['index']) == str(sample_id):\n",
    "            a = int(row['Label'])\n",
    "            if a == 0:\n",
    "                return 0 #'individual'\n",
    "            elif a == 1:\n",
    "                return 1 #'community'\n",
    "            elif a == 2:\n",
    "                return 2 # 'organization'\n",
    "    print(\"Function failed\")\n",
    "    \n",
    "training_data = pd.DataFrame(columns=['Sample', 'Text', 'Label'])\n",
    "ind = 0 # 'individual' #0\n",
    "com = 1 # 'community' #1\n",
    "org = 2 # 'organization' #2\n",
    "count = 1\n",
    "for column, row in stB_eval.iterrows():\n",
    "    sample_id = row['filename'].replace(\"/content/drive/MyDrive/CASE2023_Task4/CASE2023_TASK4_EvalData/subtaskB/\",\"\").replace(\".jpg\",\"\")\n",
    "    sample_text = preprocess_synopsis(row['text'])\n",
    "    training_data.loc[count] = [sample_id, sample_text, labeler(sample_id, eval_subtaskB)]\n",
    "    count+=1\n",
    "for column, row in community_stB_train.iterrows():\n",
    "    sample_id = row['filename'].replace(\"/content/drive/MyDrive/CASE2023_Task4/CASE2023_TASK4_TrainData/subTaskB/Community/\",\"\").replace(\".jpg\",\"\")\n",
    "    sample_text = preprocess_synopsis(row['text'])\n",
    "    training_data.loc[count] = [sample_id, sample_text, com]\n",
    "    count+=1\n",
    "for column, row in individual_stB_train.iterrows():\n",
    "    sample_id = row['filename'].replace(\"/content/drive/MyDrive/CASE2023_Task4/CASE2023_TASK4_TrainData/subTaskB/Individual/\",\"\").replace(\".jpg\",\"\")\n",
    "    sample_text = preprocess_synopsis(row['text'])\n",
    "    training_data.loc[count] = [sample_id, sample_text, ind]\n",
    "    count+=1\n",
    "for column, row in org_stB_train.iterrows():\n",
    "    sample_id = row['filename'].replace(\"/content/drive/MyDrive/CASE2023_Task4/CASE2023_TASK4_TrainData/subTaskB/Organization/\",\"\").replace(\".jpg\",\"\")\n",
    "    sample_text = preprocess_synopsis(row['text'])\n",
    "    training_data.loc[count] = [sample_id, sample_text, org]\n",
    "    count+=1\n",
    "    \n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60f1aa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sample                                               Text\n",
      "1    10125  www.bh Future Fact! @MACFacts Genius When Pout...\n",
      "2    10525  THE STANLEY KUBRICK'S SHINING HEEERE'S VLADDY ...\n",
      "3    11286  AND THEN ZELENSKY SAID... WE'VE GOT A FAKE AIR...\n",
      "4    11436  Russian troops firing at Ukrainian civilians R...\n",
      "5    12139  2 wars are happening It only is Russia, Ukrain...\n",
      "..     ...                                                ...\n",
      "238  97985             RUSSIA Me trying to live a normal life\n",
      "239  98310  WE SHOULD LOOK FOR THE WAYS TO SAVE PUTIN'S FA...\n",
      "240  99145  WE WILL NOT ESTABLISH A 'NO FLY ZONE' IN UKRAI...\n",
      "241  99821  I TRIED WARNING Y'ALL ABOUT TRUMP AND PUTIN'S ...\n",
      "242  99933  Just blame everything on Russia. imgflip.com Y...\n",
      "\n",
      "[242 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "test_datas = pd.DataFrame(columns=['Sample', 'Text'])\n",
    "count = 1\n",
    "for column, row in stB_test.iterrows():\n",
    "    sample_id = row['filename'].replace(\"/content/drive/MyDrive/CASE2023_Task4/CASE2023_TASK4_TestData/subtaskB/\",\"\").replace(\".jpg\",\"\")\n",
    "    sample_text = row['text']\n",
    "    test_datas.loc[count] = [sample_id, sample_text]\n",
    "    count+=1\n",
    "    \n",
    "print(test_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0365fa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\CHQ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.9546893810684031, Test Accuracy = 0.6210045662100456, F1 Score = 0.5532251713353605\n",
      "Epoch 2: Train Loss = 0.7840688060630452, Test Accuracy = 0.6757990867579908, F1 Score = 0.6555492660552441\n",
      "Epoch 3: Train Loss = 0.6123944539915431, Test Accuracy = 0.6872146118721462, F1 Score = 0.6730711116068794\n",
      "Epoch 4: Train Loss = 0.46432061073454944, Test Accuracy = 0.6461187214611872, F1 Score = 0.6235876474949469\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "\n",
    "# Step 1: Prepare the environment\n",
    "df = training_data\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Step 2: Tokenize the text\n",
    "def tokenize_text(text):\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=256,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoded['input_ids'], encoded['attention_mask']\n",
    "\n",
    "train_tokens = train_df['Text'].apply(tokenize_text)\n",
    "test_tokens = test_df['Text'].apply(tokenize_text)\n",
    "\n",
    "# Step 3: Prepare the data for BERT\n",
    "train_inputs = torch.cat([token[0] for token in train_tokens])\n",
    "train_masks = torch.cat([token[1] for token in train_tokens])\n",
    "train_labels = torch.tensor(train_df['Label'].values)\n",
    "\n",
    "test_inputs = torch.cat([token[0] for token in test_tokens])\n",
    "test_masks = torch.cat([token[1] for token in test_tokens])\n",
    "test_labels = torch.tensor(test_df['Label'].values)\n",
    "\n",
    "# Step 4: Fine-tune BERT for classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[0]\n",
    "        attention_mask = batch[1]\n",
    "        labels = batch[2]\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[0]\n",
    "        attention_mask = batch[1]\n",
    "        labels = batch[2]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        true_labels.extend(labels.numpy())\n",
    "        predicted_labels.extend(predicted.numpy())\n",
    "\n",
    "    accuracy = total_correct / len(test_df)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "    print(f'Epoch {epoch + 1}: Train Loss = {avg_train_loss}, Test Accuracy = {accuracy}, F1 Score = {f1}')\n",
    "\n",
    "#Epoch 1: Train Loss = 0.9558789600025523, Test Accuracy = 0.634703196347032, F1 Score = 0.566007690153761\n",
    "#Epoch 2: Train Loss = 0.7432595472444188, Test Accuracy = 0.6506849315068494, F1 Score = 0.6126640848053292\n",
    "#Epoch 3: Train Loss = 0.49651158079504965, Test Accuracy = 0.6529680365296804, F1 Score = 0.639967891094413\n",
    "#Epoch 4: Train Loss = 0.2876184206117283, Test Accuracy = 0.634703196347032, F1 Score = 0.6193736367093801"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f4fd9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text):\n",
    "    inputs = tokenize_text(text)\n",
    "    input_ids = inputs[0]\n",
    "    attention_mask = inputs[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs[0]\n",
    "\n",
    "    _, predicted_label = torch.max(logits, dim=1)\n",
    "    return predicted_label.item()\n",
    "\n",
    "predicciones_chidas = []\n",
    "for column, row in test_datas.iterrows():\n",
    "    sample = row['Sample']\n",
    "    texto = row['Text']\n",
    "    predicted_label = predict_text(texto)\n",
    "    linea = '{\"index\": '+ str(sample)+ ', \"prediction\": '+ str(predicted_label)+ '}'\n",
    "    predicciones_chidas.append(linea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "915665ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "jason = open('BERT_submissionSTB_prepros.json', 'w')\n",
    "for i in range(len(predicciones_chidas)-1):\n",
    "    jason.write(predicciones_chidas[i])\n",
    "    jason.write('\\n')\n",
    "jason.write(predicciones_chidas[-1])\n",
    "jason.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
